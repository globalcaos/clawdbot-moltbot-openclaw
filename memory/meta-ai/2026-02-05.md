# Meta-AI Research Scan â€” 2026-02-05

## Executive Summary

**Key Trends This Week:**
1. **Sparse MoE dominance** â€” Qwen3-Coder-Next (80B/3B active) achieves frontier-level coding at local-runnable scale
2. **VLA models maturing** â€” Microsoft Rho-alpha adds tactile sensing; SAM3 enables open-vocabulary video segmentation
3. **YOLO26 released** â€” Multi-task framework optimized for edge deployment (Jetson Nano/Orin)
4. **Video generation commoditizing** â€” LTX-2 is first open-source model with native audio sync
5. **Chinese models leading open-source** â€” DeepSeek V4, Qwen3, GLM-4.7 outperforming Western counterparts

---

## ğŸ§  Self-Hostable LLMs

### Qwen3-Coder-Next (Alibaba) â€” **HOT** ğŸ”¥
- **Released:** 2026-02-03
- **Architecture:** 80B MoE â†’ 3B active parameters
- **Benchmarks:** 70.6% SWE-Bench Verified, matches Claude Sonnet 4.5 on coding
- **License:** Open weights (Apache 2.0)
- **Hardware:** Runnable on high-end consumer GPU (RTX 4090/5090)
- **Scores:**
  - ğŸ”¬ Scientific: 5/5 â€” First local model closing gap with frontier on real coding tasks
  - ğŸ’° Commercial: 5/5 â€” Private code assistant, no API costs
  - âš™ï¸ Automation: 5/5 â€” Agentic coding workflows
  - ğŸ”— **SerraVision:** Direct fit for vision pipeline code generation
  - ğŸ”— **Gantry Robot Arm:** Control code generation, ROS integration

### DeepSeek-R1 / V3 (DeepSeek)
- **Architecture:** 671B MoE, 128K context
- **Benchmarks:** Matches GPT-4o, outperforms Llama 3.1 and Qwen 2.5
- **Cost:** 96% cheaper per token than ChatGPT
- **Caveat:** 400GB+ at 4-bit quant â€” needs multi-GPU or cloud
- **Scores:**
  - ğŸ”¬ Scientific: 4/5 â€” Reasoning capabilities rival closed models
  - ğŸ’° Commercial: 5/5 â€” API cost disruption
  - âš™ï¸ Automation: 3/5 â€” Too large for edge
  - ğŸ”— **All projects:** General reasoning assistant

### GLM-4.7-Flash (Z.AI)
- **Architecture:** 30B MoE â†’ 3B active
- **Specialty:** "Preserved Thinking" mode for multi-turn agentic tasks
- **Benchmarks:** SOTA for size on AIME 25, GPQA, SWE-bench
- **Deploy:** A100 or GGUF on CPU via Ollama
- **Scores:**
  - ğŸ”¬ Scientific: 4/5
  - ğŸ’° Commercial: 4/5
  - âš™ï¸ Automation: 5/5 â€” Excellent for local deployment
  - ğŸ”— **Gantry Robot Arm:** Multi-step task reasoning

### Phi-4-Mini (Microsoft)
- **Size:** 3.8B parameters
- **Best for:** Edge deployment, mobile, embedded
- **Install:** `ollama run phi4-mini`
- **Scores:**
  - ğŸ”¬ Scientific: 3/5
  - ğŸ’° Commercial: 4/5
  - âš™ï¸ Automation: 5/5 â€” Runs on laptop/embedded
  - ğŸ”— **Pallet Scan:** On-device reasoning

### MiniMax-M2.1
- **Architecture:** 229B â†’ 10B active
- **Specialty:** Full-stack app development, multilingual (beats Claude 4.5)
- **Context:** 200K tokens
- **Scores:**
  - ğŸ”¬ Scientific: 4/5
  - ğŸ’° Commercial: 5/5
  - âš™ï¸ Automation: 3/5 â€” Needs beefy hardware

---

## ğŸ‘ï¸ Vision & Object Detection

### YOLO26 (Ultralytics) â€” **CRITICAL FOR PROJECTS** ğŸ¯
- **Paper:** arXiv:2509.25164
- **Tasks:** Detection, segmentation, pose, OBB, classification
- **Edge Benchmarks:** Jetson Nano, Orin optimized
- **Export:** ONNX, TensorRT, CoreML, TFLite, INT8/FP16
- **Scores:**
  - ğŸ”¬ Scientific: 4/5 â€” Incremental but production-ready
  - ğŸ’° Commercial: 5/5 â€” Immediate deployment
  - âš™ï¸ Automation: 5/5 â€” Real-time on edge
  - ğŸ”— **SerraVision:** Primary detection backbone
  - ğŸ”— **Pallet Scan:** Pallet/object detection
  - ğŸ”— **Gantry Robot Arm:** Object localization for grasping

### SAM3 (Meta) â€” **NEW**
- **Capability:** Promptable Concept Segmentation (PCS)
- **Upgrade:** Unified image + video segmentation
- **Key Feature:** Text-prompt object tracking across video frames
- **Size:** 0.9B parameters
- **Example:** "Players in white jerseys" â†’ tracks across entire video
- **Scores:**
  - ğŸ”¬ Scientific: 5/5 â€” Major leap from SAM2
  - ğŸ’° Commercial: 5/5 â€” Video analytics, sports, surveillance
  - âš™ï¸ Automation: 4/5 â€” Needs GPU but efficient
  - ğŸ”— **SerraVision:** Open-vocabulary plant segmentation
  - ğŸ”— **Pallet Scan:** Segment pallets by description
  - ğŸ”— **Gantry Robot Arm:** Object isolation for manipulation

### YOLC (Dynamic Sparse Attention)
- **Paper:** Scientific Reports 2026
- **Specialty:** High-speed small target detection for wearables
- **Use case:** Resource-constrained embedded systems
- **Scores:**
  - ğŸ”¬ Scientific: 3/5
  - ğŸ’° Commercial: 4/5
  - âš™ï¸ Automation: 5/5
  - ğŸ”— **Pallet Scan:** Small defect detection

---

## ğŸ¬ Video Generation (Open Source)

### LTX-2 (Lightricks) â€” **BREAKTHROUGH**
- **Feature:** First open-source video model with native audio sync
- **Significance:** "Stable Diffusion moment" for video
- **Implications:** Stock footage industry disruption
- **Scores:**
  - ğŸ”¬ Scientific: 5/5
  - ğŸ’° Commercial: 5/5 â€” Content generation
  - âš™ï¸ Automation: 3/5 â€” Heavy compute
  - ğŸ”— **SerraVision:** Marketing video generation

### HunyuanVideo-I2V (Tencent)
- **Strength:** Cinematic quality, coherent motion
- **Mode:** Image-to-video workflows
- **Performance:** Beats Runway Gen-3

### Mochi 1 (Genmo)
- **Strength:** High-fidelity short clips
- **Method:** Diffusion-based, strong prompt alignment

### LingBot-World
- **Type:** Open-source world model (vs. DeepMind Genie 3)
- **Released:** January 2026, full weights available
- **Feature:** Real-time interactive environment generation

---

## ğŸ—ï¸ 3D Reconstruction

### PLANING Framework
- **Paper:** ResearchGate 2026
- **Method:** Loosely coupled Triangle-Gaussian for streaming 3D
- **Applications:** UAV navigation, airborne mapping, mesh/NeRF rendering
- **Open source:** GitHub (code + datasets)
- **Scores:**
  - ğŸ”¬ Scientific: 4/5
  - ğŸ’° Commercial: 4/5 â€” Industrial mapping
  - âš™ï¸ Automation: 4/5
  - ğŸ”— **SerraVision:** Greenhouse 3D mapping
  - ğŸ”— **Pallet Scan:** Warehouse spatial awareness

### 3DGS-Enhancer
- **Method:** View-consistent 2D diffusion priors for unbounded 3DGS
- **Published:** NeurIPS 2024, implementations maturing

### Gaussian Splatting + Defense Training
- **Virtuix Virtual Terrain Walk:** 360Â° cameras â†’ photorealistic navigable 3D
- **Application:** Training/simulation environments

---

## ğŸ¤– Robotics & VLA Models

### Microsoft Rho-alpha â€” **NEW**
- **Type:** VLA+ (Vision-Language-Action + tactile)
- **Base:** Phi series VLM
- **Key Innovation:** Tactile sensing integration, force sensing planned
- **Training:** Synthetic data via Isaac Sim + RL
- **Access:** Research Early Access Program
- **Scores:**
  - ğŸ”¬ Scientific: 5/5 â€” Multimodal foundation for manipulation
  - ğŸ’° Commercial: 4/5 â€” Enterprise robotics
  - âš™ï¸ Automation: 4/5 â€” Azure deployment
  - ğŸ”— **Gantry Robot Arm:** PRIMARY RELEVANCE â€” bimanual manipulation from natural language

### IMLE Policy
- **Paper:** arXiv:2502.12371
- **Method:** Implicit Maximum Likelihood for visuomotor policy
- **Comparison:** 10x faster than Diffusion Policy, outperforms Flow Matching
- **Scores:**
  - ğŸ”¬ Scientific: 4/5
  - ğŸ’° Commercial: 4/5
  - âš™ï¸ Automation: 5/5 â€” Sample efficient
  - ğŸ”— **Gantry Robot Arm:** Policy learning for manipulation

### VLA Survey Paper
- **arXiv:** 2505.04769
- **Content:** Comprehensive overview of VLA progress, applications, challenges
- **GitHub:** Reference implementations available

### MoveIt (ROS)
- **Status:** Gold standard for robot manipulation planning
- **Features:** Motion planning, trajectory optimization, collision avoidance
- **Scores:**
  - ğŸ”¬ Scientific: 3/5 â€” Mature
  - ğŸ’° Commercial: 5/5
  - âš™ï¸ Automation: 5/5
  - ğŸ”— **Gantry Robot Arm:** Integration target

---

## ğŸ–¼ï¸ Image Generation

### FLUX 2 Pro (Black Forest Labs)
- **Status:** Dominates professional image gen in 2026
- **Architecture:** 16-channel latent space (vs SD's 4)
- **Created by:** Former Stability AI researchers
- **Local:** ComfyUI, Recraft Studio, SD WebUI Forge

### Stable Diffusion 3.5
- **Status:** Still viable for fine-tuning, LoRA workflows
- **Best for:** Controlled generation with custom models

---

## ğŸ“Š Benchmark Updates

| Model | SWE-Bench Verified | Arena Hard | Notes |
|-------|-------------------|------------|-------|
| Qwen3-Coder-Next | 70.6% | â€” | 3B active params |
| Qwen3-30B-A3B | 69.6% | 91.0 | MoE variant |
| Claude Opus 4.5 | ~68% | â€” | Reference point |
| DeepSeek-R1 | ~65% | â€” | MIT license |
| GLM-4.7-Flash | SOTA for size | â€” | 3B active |

---

## ğŸ¯ Project Relevance Matrix

| Finding | SerraVision | Pallet Scan | Gantry Robot Arm |
|---------|-------------|-------------|------------------|
| YOLO26 | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| SAM3 | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| Qwen3-Coder-Next | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| Rho-alpha | â­â­ | â­â­ | â­â­â­â­â­ |
| PLANING (3DGS) | â­â­â­â­ | â­â­â­ | â­â­â­ |
| IMLE Policy | â­ | â­ | â­â­â­â­â­ |
| GLM-4.7-Flash | â­â­â­ | â­â­â­ | â­â­â­â­ |
| LTX-2 Video | â­â­â­ | â­ | â­ |

---

## ğŸš€ Immediate Action Items

### High Priority (This Week)
1. **Evaluate YOLO26** â€” Download, benchmark on existing pallet dataset
   ```bash
   pip install ultralytics
   yolo detect train model=yolo26n.pt data=pallets.yaml
   ```

2. **Test SAM3** for open-vocab segmentation
   ```bash
   pip install segment-anything-3
   # Try: "wooden pallet", "cardboard box", "forklift"
   ```

3. **Set up Qwen3-Coder-Next** for code assistance
   ```bash
   ollama run qwen3-coder-next
   ```

### Medium Priority (This Month)
4. **Review Rho-alpha** papers for Gantry integration patterns
5. **Benchmark IMLE Policy** vs current manipulation approach
6. **Prototype SAM3 tracking** for video-based inventory

### Research Track
7. **Monitor DeepSeek V4** release (expected Q2 2026)
8. **Track GLM-5** development (Z.AI roadmap)
9. **Watch for Qwen3.5** improvements

---

## ğŸ“š Key Papers to Read

1. [YOLO26: Key Architectural Enhancements](https://arxiv.org/abs/2509.25164)
2. [VLA Models Survey](https://arxiv.org/abs/2505.04769)
3. [IMLE Policy: Fast Visuomotor Learning](https://arxiv.org/html/2502.12371)
4. [Qwen3-ASR Technical Report](https://arxiv.org/html/2601.21337)
5. [PLANING: Streaming 3D Reconstruction](https://www.researchgate.net/publication/400236734)

---

## ğŸ’¡ Strategic Observations

1. **The MoE efficiency revolution is real** â€” 3B active params now match 70B dense models on specialized tasks
2. **Chinese labs are shipping faster** â€” Qwen, DeepSeek, GLM releasing weekly; Western labs slower
3. **Multimodal is table stakes** â€” Every new model handles vision+language minimum
4. **Tactile/force sensing emerging** â€” Rho-alpha signals next wave of robotics foundation models
5. **Video-with-audio is the new frontier** â€” LTX-2 just opened the floodgates

---

*Scan completed 2026-02-05 07:48 CET*
*Next scheduled scan: 2026-02-12*
